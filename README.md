# Crawling Engine

자격증 일정·시험 정보·CBT 데이터를 수집하기 위한 Python 기반 크롤링 엔진입니다.  
본 엔진은 졸업작품의 핵심 구성 요소로, 공공 자격증(큐넷)과 민간 자격증 사이트의
서로 다른 구조를 고려해 확장성과 운영 안정성을 중심으로 설계되었습니다.

## 프로젝트 개요
- 자격증 정보가 여러 사이트에 분산되어 있어 관리가 어렵다는 문제에서 출발
- 공공/민간 자격증 데이터를 표준 JSON 구조로 통합
- 실패 제어와 재처리를 고려한 크롤링 파이프라인 설계

## 전체 구조
fetch → parse → normalize → DB → API → UI

## 설계 포인트
- 공공 / 민간 자격증 파이프라인 분리
- adapter / util 계층을 통한 규칙 분리
- 완벽한 자동화보다 운영 가능성과 데이터 품질을 우선

## 코드 읽기 가이드
- engine_common : 공통 크롤링 파이프라인
- public_cert_api : 공공 자격증(큐넷) 처리
- private-cert-crawl : 민간 자격증 처리


---

## 상세 설계 배경 및 문제 해결 과정

### 1. 프로젝트 시작 배경

큐넷, 자격증넷 등 기존 자격증 사이트들은 일정, 시험 정보, CBT 기능이 각각 분산되어 있어,
사용자가 관심 있는 자격증만을 모아 한눈에 관리하기 어렵다는 불편함이 있었습니다.
팀원들과 논의한 끝에 자격증 일정·시험 정보·CBT·즐겨찾기 기능을 하나의 서비스로 통합해
제공하자는 결론에 도달했고, 이를 졸업작품 주제로 선정했습니다.

---

### 2. 문제 ① 크롤링 파이프라인 설계

가장 처음 마주한 문제는 크롤링 자체가 아니라,
**“인터넷에 흩어진 정보를 어떤 구조로 수집·정리할 것인가”**였습니다.

이에 따라  
fetch(HTML 수집) → parse(JSON 변환) → normalize(정규화 및 DB 적재)  
라는 공통 파이프라인을 기준으로 전체 구조를 설계했습니다.

또한 사이트별 HTML 규칙 차이를 흡수하기 위해 adapter 계층을 두고,
불필요한 문자 제거·빈 값 처리·형식 통일과 같은 공통 작업은
util 스크립트로 분리해 관리했습니다.

---

### 3. 문제 ② 공공·민간 자격증 데이터 구조 차이

공공 자격증(큐넷)과 민간 자격증 사이트는 데이터 구조와 신뢰도 측면에서 성격이 크게 달랐습니다.

이 문제를 해결하기 위해 공공 자격증과 민간 자격증을
동일한 규칙으로 처리하지 않고, 파이프라인 수준에서 분리하는 설계를 선택했습니다.

민간 자격증은 비교적 구조가 안정적인 만큼 자동화 비중을 높였고,
큐넷의 경우에는 자동화 가능한 영역과 수작업 검증이 필요한 영역을 구분해 접근했습니다.

그 결과 약 80~90% 수준의 데이터 정합성을 확보했고,
완벽한 자동화보다 운영 가능성과 데이터 품질을 우선하는 방향으로
구현 범위를 설정했습니다.
